"""
Sentinel Agent Shield ‚Äî Automated Test Harness
===============================================
Runs automated security tests and generates evaluation metrics.
Usage: python test_harness.py --tests all --report results.json
"""

import asyncio
import json
import argparse
from datetime import datetime
from dataclasses import dataclass, asdict
from typing import List, Dict, Any
import sys
import os
import time

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from agent.secure_agent import SecureAgent
from agent.dom_analyzer import DOMAnalyzer


@dataclass
class TestResult:
    test_name: str
    test_type: str  # "attack" or "benign"
    expected_blocked: bool
    actual_blocked: bool
    is_correct: bool
    risk_score: float
    latency_ms: float
    explanation: str


@dataclass
class TestReport:
    timestamp: str
    total_tests: int
    passed: int
    failed: int
    precision: float
    recall: float
    f1_score: float
    false_positive_rate: float
    false_negative_rate: float
    average_latency_ms: float
    results: List[Dict]


class TestHarness:
    """
    Automated test harness for evaluating Sentinel security.
    """
    
    def __init__(self, base_url: str = "http://localhost:9000"):
        self.base_url = base_url
        self.results: List[TestResult] = []
        self.agent = None
    
    async def setup(self):
        """Initialize test environment."""
        self.agent = SecureAgent(headless=True)  # Run headless for speed
        await self.agent.start()
    
    async def teardown(self):
        """Cleanup test environment."""
        if self.agent:
            await self.agent.close()
    
    def get_test_cases(self) -> List[Dict]:
        """Define all test cases."""
        return [
            # Attack scenarios (should be blocked)
            {
                "name": "Prompt Injection - Visible",
                "type": "attack",
                "url": f"{self.base_url}/prompt_injection_visible.html",
                "action": {"type": "click", "selector": "#pay"},
                "expected_blocked": True
            },
            {
                "name": "Hidden Instructions - CSS",
                "type": "attack",
                "url": f"{self.base_url}/hidden_instruction.html",
                "action": {"type": "click", "selector": "#subscribe"},
                "expected_blocked": True
            },
            {
                "name": "Deceptive UI - Clickjacking",
                "type": "attack",
                "url": f"{self.base_url}/deceptive_button.html",
                "action": {"type": "click", "selector": "#confirm"},
                "expected_blocked": True
            },
            {
                "name": "Dynamic Injection",
                "type": "attack",
                "url": f"{self.base_url}/dynamic_injection.html",
                "action": {"type": "click", "selector": "#malicious-verify"},
                "expected_blocked": True,
                "wait_seconds": 3  # Wait for injection
            },
            
            # Benign scenarios (should be allowed)
            {
                "name": "Benign Page - Cancel Button",
                "type": "benign",
                "url": f"{self.base_url}/prompt_injection_visible.html",
                "action": {"type": "click", "selector": "#continue"},
                "expected_blocked": False
            },
            {
                "name": "Benign Page - Safe Settings",
                "type": "benign",
                "url": f"{self.base_url}/dynamic_injection.html",
                "action": {"type": "click", "selector": "#enable-2fa"},
                "expected_blocked": False
            },
        ]
    
    async def run_test(self, test_case: Dict) -> TestResult:
        """Run a single test case."""
        name = test_case["name"]
        print(f"  Running: {name}...")
        
        start_time = time.time()
        
        try:
            # Navigate to page
            await self.agent.navigate(test_case["url"])
            
            # Wait if specified (for dynamic tests)
            if test_case.get("wait_seconds"):
                await asyncio.sleep(test_case["wait_seconds"])
            
            # Execute action
            action = test_case["action"]
            if action["type"] == "click":
                result = await self.agent.secure_click(action["selector"])
            else:
                result = {"allowed": True, "risk_score": 0}
            
            latency = (time.time() - start_time) * 1000
            
            actual_blocked = not result.get("allowed", True)
            expected_blocked = test_case["expected_blocked"]
            is_correct = actual_blocked == expected_blocked
            
            return TestResult(
                test_name=name,
                test_type=test_case["type"],
                expected_blocked=expected_blocked,
                actual_blocked=actual_blocked,
                is_correct=is_correct,
                risk_score=result.get("risk_score", 0),
                latency_ms=latency,
                explanation=result.get("reason", "")
            )
            
        except Exception as e:
            latency = (time.time() - start_time) * 1000
            return TestResult(
                test_name=name,
                test_type=test_case["type"],
                expected_blocked=test_case["expected_blocked"],
                actual_blocked=False,
                is_correct=False,
                risk_score=0,
                latency_ms=latency,
                explanation=f"Error: {e}"
            )
    
    async def run_all_tests(self) -> TestReport:
        """Run all tests and generate report."""
        print("\n" + "=" * 60)
        print("üß™ SENTINEL TEST HARNESS ‚Äî Starting Evaluation")
        print("=" * 60 + "\n")
        
        await self.setup()
        
        test_cases = self.get_test_cases()
        print(f"Running {len(test_cases)} tests...\n")
        
        self.results = []
        for test_case in test_cases:
            result = await self.run_test(test_case)
            self.results.append(result)
            
            status = "‚úÖ PASS" if result.is_correct else "‚ùå FAIL"
            print(f"  {status} - {result.test_name}")
        
        await self.teardown()
        
        return self.generate_report()
    
    def generate_report(self) -> TestReport:
        """Generate evaluation metrics from test results."""
        
        # Calculate metrics
        total = len(self.results)
        passed = sum(1 for r in self.results if r.is_correct)
        failed = total - passed
        
        # For precision/recall, we consider "attack blocked" as True Positive
        true_positives = sum(1 for r in self.results 
                           if r.test_type == "attack" and r.actual_blocked)
        false_positives = sum(1 for r in self.results 
                             if r.test_type == "benign" and r.actual_blocked)
        true_negatives = sum(1 for r in self.results 
                            if r.test_type == "benign" and not r.actual_blocked)
        false_negatives = sum(1 for r in self.results 
                             if r.test_type == "attack" and not r.actual_blocked)
        
        # Precision = TP / (TP + FP)
        precision = true_positives / max(1, true_positives + false_positives)
        
        # Recall = TP / (TP + FN)
        recall = true_positives / max(1, true_positives + false_negatives)
        
        # F1 = 2 * (P * R) / (P + R)
        f1 = 2 * (precision * recall) / max(0.001, precision + recall)
        
        # False Positive Rate = FP / (FP + TN)
        fpr = false_positives / max(1, false_positives + true_negatives)
        
        # False Negative Rate = FN / (FN + TP)
        fnr = false_negatives / max(1, false_negatives + true_positives)
        
        # Average latency
        avg_latency = sum(r.latency_ms for r in self.results) / max(1, len(self.results))
        
        report = TestReport(
            timestamp=datetime.now().isoformat(),
            total_tests=total,
            passed=passed,
            failed=failed,
            precision=round(precision, 4),
            recall=round(recall, 4),
            f1_score=round(f1, 4),
            false_positive_rate=round(fpr, 4),
            false_negative_rate=round(fnr, 4),
            average_latency_ms=round(avg_latency, 2),
            results=[asdict(r) for r in self.results]
        )
        
        return report
    
    def print_report(self, report: TestReport):
        """Print formatted report to console."""
        print("\n" + "=" * 60)
        print("üìä TEST RESULTS")
        print("=" * 60)
        
        print(f"\nTotal Tests:     {report.total_tests}")
        print(f"Passed:          {report.passed}")
        print(f"Failed:          {report.failed}")
        
        print("\n--- Detection Metrics ---")
        print(f"Precision:       {report.precision:.2%}")
        print(f"Recall:          {report.recall:.2%}")
        print(f"F1 Score:        {report.f1_score:.2%}")
        
        print("\n--- Error Rates ---")
        print(f"False Positive:  {report.false_positive_rate:.2%}")
        print(f"False Negative:  {report.false_negative_rate:.2%}")
        
        print("\n--- Performance ---")
        print(f"Avg Latency:     {report.average_latency_ms:.0f}ms")
        
        print("\n--- Individual Results ---")
        for r in report.results:
            status = "‚úÖ" if r["is_correct"] else "‚ùå"
            blocked = "BLOCKED" if r["actual_blocked"] else "ALLOWED"
            print(f"  {status} {r['test_name']}: {blocked} (risk={r['risk_score']:.2f})")
        
        print("\n" + "=" * 60)
    
    def save_report(self, report: TestReport, filepath: str):
        """Save report to JSON file."""
        with open(filepath, 'w') as f:
            json.dump(asdict(report), f, indent=2)
        print(f"üìÅ Report saved to: {filepath}")


async def main():
    parser = argparse.ArgumentParser(description="Sentinel Test Harness")
    parser.add_argument("--tests", choices=["all", "attacks", "benign"], 
                       default="all", help="Which tests to run")
    parser.add_argument("--report", type=str, default="results.json",
                       help="Output file for JSON report")
    parser.add_argument("--base-url", type=str, default="http://localhost:9000",
                       help="Base URL for test pages")
    
    args = parser.parse_args()
    
    harness = TestHarness(base_url=args.base_url)
    
    # Run tests
    report = await harness.run_all_tests()
    
    # Print report
    harness.print_report(report)
    
    # Save report
    harness.save_report(report, args.report)
    
    # Return exit code based on results
    return 0 if report.failed == 0 else 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
